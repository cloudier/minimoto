COMP9243 Distributed Systems
Assignment 3
Group 12

===============================================
Application architecture

The application architecture is client-server, where the 'server' is a
distributed system made up of multiple servers that are transparent to the
client. To convert a series of images into a video, the client first uploads
the images into the input S3 bucket. The images for a particular video must be
in one directory, and the directory is suffixed with a string of random
ASCII characters so that multiple conversion requests with the same directory do
not interfere with each other. Then, the client adds a message to the SQS queue
with the name of the directory and the random suffix. The conversion is complete
when the SQS message has been deleted and the resulting video is available in
the output S3 bucket.

The application that provides the service is run periodically via cron. If the
instance it is running on isn't currently converting a video, it will grab a new
message from the SQS queue and start converting that.

The client, watchdog and service are set up by the setup script. This setup
script:
  - Creates a new IAM role and instance profile to provide the new instances
    access to IAM, EC2, SQS and S3.
  - Creates an SQS queue.
  - Creates the input and output S3 buckets.
  - Creates a security group for the new instances to allow ssh access.
  - Starts up the client, watchdog and service instances, and uploads files to
    these instances using scp.
  - Creates an AMI for the watchdog to use to setup new service instances.
The clean up script cleans up the resources and instances created by the setup
script, as well as anything created by the watchdog. It receives this
information from the setup script via a pickle file. This could be replaced by
using tags to mark the instances the cleanup script should terminate. The tags
would be stored in `minimoto_constants.py`.

The watchdog creates and destroys instances depending on their CPU utilisation
and the number of messages in the SQS queue.

===============================================
Algorithms

-----------------------------------------------
Transcoding service

The transcoding service follows the algorithm given in the spec.
While the transcoding service is running, it creates a file called
MINIMOTO_SERVICE_RUNNING. When the service starts, it checks for the presense of
this file before continuing.
We make sure to only delete a message from the SQS queue once a transcoding
request is complete and the output has been successfully uploaded to the output
s3 bucket. This means that if transcoding fails for some reason, the message
will be automatically placed back into the queue by SQS. We set the visibility
timeout to 60 seconds. This is long enough that a transcoding operation should
finish in that time, otherwise we risk having two instances working on the same
request, which is wasteful.
When the service finishes, we delete the images and the created video from the
instance as well as the images from the input s3 bucket.

-----------------------------------------------
Watchdog

The watchdog checks up on service instances using CloudWatch. It identifies
service instances using a tag that is assigned on instance creation. (This tag
is defined in `minimoto_constants.py`.) When launched with the `--status` flag,
the watchdog prints out the status and CPU utilisation of each instance, as well
as the average utilisation and the number of messages in the queue. Otherwise,
the watchdog checks the number of messages in the queue. If there are messages
but no instances, then the watchdog starts up an instance. If there are
instances, then the watchdog will only start up more instances if there are
more than 5 messages in the queue. The watchdog aims to keep the number of
instances running equal to the number of pending messages / 5.
If there are fewer than 5 messages, then the watchdog will kill instances with a
CPU utilisation that is lower than 10%. The watchdog will only kill instances
that are less than 10 minutes from the end of their current billing cycle.

===============================================
Code

-----------------------------------------------
Language, Libraries and Tools

We wrote our scripts in Python 3 and used the boto3 library.

-----------------------------------------------
Structure

`minimoto_constants.py`: shared names and domains
`minimoto_default_userdata`: we run this script when starting up the client and
  watchdog instance. It installs pip3, boto3 and awscli.
`minimoto_service_userdata`: we run this script whenever we start up a service
  instance. It installs pip3, boto3, awscli and avconv-related applications, and
  sets up cron.
`Makefile`: installs pip3, boto3 and awscli, and ensures that img2video and our
  Python scripts are executable.
`iam_trust_policy.json`: used to set up new EC2 instances with the correct
  permissions for accessing IAM, EC2, SQS and S3.
Every other file is as described in the spec

===============================================
Testing

We tested the basic functionality of our scripts by manually starting up an EC2
instance, running the setup script and then ssh-ing into the client, service and
watchdog instances. Then we ran these scripts and checked the results in AWS to
test different aspects of their functionality. For example, if we ran the client
script we would expect the input bucket to contain the uploaded images and the
SQS queue to contain one message. This was mainly used during development.

We tested a large single transcoding job by uploading 60 images of various
sizes. An m4.large instance was sufficient to complete the transcoding in less
than a minute. We tried a larger number of images, 100, but the transcoding
always failed in the convert -morph step due to low memory.

We also tested a large number of concurrent requests by creating 20 transcoding
requests at once. While they were pending, we ran the watchdog to test the
scaling functionality. Once it had spun up more instances we tested the watchdog
status reporting functionality. While the requests were still being serviced, we
ran the watchdog again to make sure it didn't scale up any more. Once all the
transcoding was complete we ran the watchdog to make sure it scaled down.

===============================================
Shortcomings

-----------------------------------------------
Security

We could improve the security of our application by having multiple instance
profiles. Our solution uses only one instance profile, which means that it has
full access to EC2, SQS, S3 and IAM. The watchdog only needs IAM and EC2 access,
whereas the client and service only requires SQS and S3 permissions.
Furthermore, we provided full access to each of the services but we could have
used a more restricted set of permissions to each service.

We also pass the keyfile from the setup script to the watchdog script. We
could instead have the watchdog generate a new keyfile for the service instances
it starts up, but we thought that this would be inconvenient for the marker.

Ideally, we would also protect sudo privileges on our EC2 instances with some
sort of password. We believe that in this assignment, the inconvenience of this
would outweigh the security benefit, given that no sensitive information is
stored on the instances and the instances don't stay up for very long.

-----------------------------------------------
Performance

We could improve the performance of our service by checking if there are
multiple cores on the instance the service is running on and then ensuring that
there is one transcoding process for each core.

-----------------------------------------------
Cost

We leave idle instances running if they have only been running for part of an
hour, since AWS bills running instances per hour. We might be able to cut more
costs if we stop (not terminate) idle instances instead.

-----------------------------------------------
DoS

When processing a large number of images, the transcoding process can crash and
will be restarted. A malicious user could create many requests with large
numbers of images that would consistently fail and be restarted. This could be
fixed by limiting the size/number of images allowed in each request.
